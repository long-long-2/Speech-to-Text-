import torch
import sounddevice as sd
import numpy as np
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
from transformers.pipelines.automatic_speech_recognition import AutomaticSpeechRecognitionPipeline

# === C·∫§U H√åNH LOCAL MODEL ===
cache_dir = "direction/of/PhoWhisper"
model_id = "vinai/PhoWhisper-tiny"

# === LOAD T·ª™ LOCAL CACHE ===
processor = AutoProcessor.from_pretrained(model_id, cache_dir=cache_dir, local_files_only=True)
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, cache_dir=cache_dir, local_files_only=True)

asr = AutomaticSpeechRecognitionPipeline(model=model, processor=processor)

# === AUDIO CONFIG ===
samplerate = 16000
channels = 1
record_duration = 5
recognized_texts = []

print(" Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?...")

def record_and_transcribe():
    print("\nüéôÔ∏è Ghi √¢m...")
    recording = sd.rec(
        int(record_duration * samplerate),
        samplerate=samplerate,
        channels=channels,
        dtype='float32'
    )
    sd.wait()

    # √âp th√†nh 1D array (n·∫øu b·ªã shape (N, 1))
    audio = np.squeeze(recording)

    result = asr(audio, generate_kwargs={"language": "vi"})
    text = result["text"]
    print("üìÑ B·∫°n n√≥i: ", text)
    recognized_texts.append(text)

try:
    while True:
        record_and_transcribe()

except KeyboardInterrupt:
    print("\nüõë ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh.")
    with open("output.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(recognized_texts))
    print("‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: output.txt")
